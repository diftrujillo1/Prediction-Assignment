---
title: "Prediction Assignment"
author: "Diego Trujillo"
date: "September 25, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### First Details

To recreate this project, just clone or download this repo in a PC and open the R project in it called *_Project_*. This will set you the current working directory _(cwd)_ in wherever place you have download the project. 

There are two datasets that must be in the whole project, same structure and same path: `pml-testing.csv`and `pml-training.csv`. 

### Exploratory Analysis

First I loaded the datasets to see which data types where on it, which variables has missing values for almost all the entire dataset. I noticed that _'#DIV/0!'_, _'NA'_ where also missing values when reading it, so it must be said to the function while importing the datasets.

```{r}
suppressWarnings(suppressMessages(library(tidyverse)))
suppressWarnings(suppressMessages(library(caret)))
suppressWarnings(suppressMessages(library(randomForest)))
suppressWarnings(suppressMessages(library(e1071)))
```

Another important thing is that it is better to deal with no missing values inside the dataset, so it's important to drop every variable in the dataset that has missing values. When droping variables, new training dataset came with 57 variables, instead of 160 and testing dataset with 60 variables instead of 160.

```{r}
train_set <- read.csv('pml-training.csv', na.strings = c('#DIV/0!', 'NA', ''))
train_set <- train_set[,colSums(is.na(train_set)) == 0]

test_set <- read.csv('pml-testing.csv', na.strings = c('#DIV/0!', 'NA', ''))
test_set <- test_set[,colSums(is.na(test_set)) == 0]
```

There are some variables that I consider as non-predictors variables, this are ids or variables time that are dropped from the _trainin\_set_. This variables are: _index_, _user\_name_, _time\_stamp_, _new\_window_ and _num\_window_. One can use the grepl function if one knows before that these variables doesn't give any extra information to the training model.

```{r}
train   <- train_set[,-c(1:7)]
test   <- test_set[,-c(1:7)]
```

Now, the cleaned training data set contains 19622 observations and 53 variables, while the testing data set contains 20 observations and 53 variables. The "classe" variable is still in the cleaned training set.

### Partitionning the Data

I created a Data Partition with the train dataset after cleansing it. Partitioning it with 70% for training test and 30% for the testing test. This testing set is different from the one loaded before. The testing set from Data Partition is for the Cross Validation section. The other one is for predicting the results of the quiz. 

```{r}
data_train <- createDataPartition(train$classe, p = 0.7, list = FALSE)
subset_train <- train[data_train,]
subset_test <- train[-data_train,]
```

### Predicting and Modeling

I defined that the model to use was Random Forest which is the best I know. It is a classification model to try to predict as best as it can the _classe_ variable. I defined a Cross Validation of 5 _'cv = 5'_. If one can reproduce this one must set a seed.

```{r}
control <- trainControl(method = "cv", 5)

model <- train(classe ~ ., data = subset_train, 
               method = "rf", 
               trControl = control, 
               ntree = 250)

prediction <- predict(model, subset_test)
confusionMatrix(subset_test$classe, prediction)
```

The model gives us an accuracy of `r postResample(prediction, subset_test$classe)[1]` or `r paste0(round(postResample(prediction, subset_test$classe)[1] * 100, 2), '%')` and an estimated out-of-sample of `r 1 - as.numeric(confusionMatrix(subset_test$classe, prediction)$overall[1])` or `r paste0(round((1 - as.numeric(confusionMatrix(subset_test$classe, prediction)$overall[1])) * 100, 2), '%')`. Which is very good.


#### Predicting Quiz set

Now apply the model to the _test\_set_ (the one that was imported from the csv).

```{r}
result <- predict(model, test_set[, -length(names(test_set))])
result
```

### Considerations

One can considerate to improve performance doing it in parallel or comparing multiple models output to determine which one is the fits the best. 


### Acknowledgement

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

http://groupware.les.inf.puc-rio.br/har#sbia_paper_section
