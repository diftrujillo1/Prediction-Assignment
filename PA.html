


<div id="first-details" class="section level3">
<h3>First Details</h3>
<p>To recreate this project, just clone or download this repo in a PC and open the R project in it called <em><em>Project</em></em>. This will set you the current working directory <em>(cwd)</em> in wherever place you have download the project.</p>
<p>There are two datasets that must be in the whole project, same structure and same path: <code>pml-testing.csv</code>and <code>pml-training.csv</code>.</p>
</div>
<div id="exploratory-analysis" class="section level3">
<h3>Exploratory Analysis</h3>
<p>First I loaded the datasets to see which data types where on it, which variables has missing values for almost all the entire dataset. I noticed that <em>‘#DIV/0!’</em>, <em>‘NA’</em> where also missing values when reading it, so it must be said to the function while importing the datasets.</p>
<pre class="r"><code>suppressWarnings(suppressMessages(library(tidyverse)))
suppressWarnings(suppressMessages(library(caret)))
suppressWarnings(suppressMessages(library(randomForest)))
suppressWarnings(suppressMessages(library(e1071)))</code></pre>
<p>Another important thing is that it is better to deal with no missing values inside the dataset, so it’s important to drop every variable in the dataset that has missing values. When droping variables, new training dataset came with 57 variables, instead of 160 and testing dataset with 60 variables instead of 160.</p>
<pre class="r"><code>train_set &lt;- read.csv(&#39;pml-training.csv&#39;, na.strings = c(&#39;#DIV/0!&#39;, &#39;NA&#39;, &#39;&#39;))
train_set &lt;- train_set[,colSums(is.na(train_set)) == 0]

test_set &lt;- read.csv(&#39;pml-testing.csv&#39;, na.strings = c(&#39;#DIV/0!&#39;, &#39;NA&#39;, &#39;&#39;))
test_set &lt;- test_set[,colSums(is.na(test_set)) == 0]</code></pre>
<p>There are some variables that I consider as non-predictors variables, this are ids or variables time that are dropped from the <em>trainin_set</em>. This variables are: <em>index</em>, <em>user_name</em>, <em>time_stamp</em>, <em>new_window</em> and <em>num_window</em>. One can use the grepl function if one knows before that these variables doesn’t give any extra information to the training model.</p>
<pre class="r"><code>train   &lt;- train_set[,-c(1:7)]
test   &lt;- test_set[,-c(1:7)]</code></pre>
<p>Now, the cleaned training data set contains 19622 observations and 53 variables, while the testing data set contains 20 observations and 53 variables. The “classe” variable is still in the cleaned training set.</p>
</div>
<div id="partitionning-the-data" class="section level3">
<h3>Partitionning the Data</h3>
<p>I created a Data Partition with the train dataset after cleansing it. Partitioning it with 70% for training test and 30% for the testing test. This testing set is different from the one loaded before. The testing set from Data Partition is for the Cross Validation section. The other one is for predicting the results of the quiz.</p>
<pre class="r"><code>data_train &lt;- createDataPartition(train$classe, p = 0.7, list = FALSE)
subset_train &lt;- train[data_train,]
subset_test &lt;- train[-data_train,]</code></pre>
</div>
<div id="predicting-and-modeling" class="section level3">
<h3>Predicting and Modeling</h3>
<p>I defined that the model to use was Random Forest which is the best I know. It is a classification model to try to predict as best as it can the <em>classe</em> variable. I defined a Cross Validation of 5 <em>‘cv = 5’</em>. If one can reproduce this one must set a seed.</p>
<pre class="r"><code>control &lt;- trainControl(method = &quot;cv&quot;, 5)

model &lt;- train(classe ~ ., data = subset_train, 
               method = &quot;rf&quot;, 
               trControl = control, 
               ntree = 250)

prediction &lt;- predict(model, subset_test)
confusionMatrix(subset_test$classe, prediction)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1674    0    0    0    0
##          B    6 1128    5    0    0
##          C    0    4 1022    0    0
##          D    0    0   13  951    0
##          E    0    0    0    1 1081
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9951          
##                  95% CI : (0.9929, 0.9967)
##     No Information Rate : 0.2855          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9938          
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9964   0.9965   0.9827   0.9989   1.0000
## Specificity            1.0000   0.9977   0.9992   0.9974   0.9998
## Pos Pred Value         1.0000   0.9903   0.9961   0.9865   0.9991
## Neg Pred Value         0.9986   0.9992   0.9963   0.9998   1.0000
## Prevalence             0.2855   0.1924   0.1767   0.1618   0.1837
## Detection Rate         0.2845   0.1917   0.1737   0.1616   0.1837
## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
## Balanced Accuracy      0.9982   0.9971   0.9909   0.9982   0.9999</code></pre>
<p>The model gives us an accuracy of 0.9950722 or 99.51% and an estimated out-of-sample of 0.0049278 or 0.49%. Which is very good.</p>
<div id="predicting-quiz-set" class="section level4">
<h4>Predicting Quiz set</h4>
<p>Now apply the model to the <em>test_set</em> (the one that was imported from the csv).</p>
<pre class="r"><code>result &lt;- predict(model, test_set[, -length(names(test_set))])
result</code></pre>
<pre><code>##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E</code></pre>
</div>
</div>
<div id="considerations" class="section level3">
<h3>Considerations</h3>
<p>One can considerate to improve performance doing it in parallel or comparing multiple models output to determine which one is the fits the best.</p>
</div>
<div id="acknowledgement" class="section level3">
<h3>Acknowledgement</h3>
<p>Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human ’13) . Stuttgart, Germany: ACM SIGCHI, 2013.</p>
<p><a href="http://groupware.les.inf.puc-rio.br/har#sbia_paper_section" class="uri">http://groupware.les.inf.puc-rio.br/har#sbia_paper_section</a></p>
</div>
